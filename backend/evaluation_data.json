[
    {
      "id": "transformer_architecture_q1",
      "question": "What are the main components of the Transformer encoder layer?",
      "document_id": 123,
      "ground_truth_chunk_ids": [
        "chunk_3a2b",
        "chunk_8c1d"
      ]
    },
    {
      "id": "attention_mechanism_q2",
      "question": "How does scaled dot-product attention differ from additive attention?",
      "document_id": 123,
      "ground_truth_chunk_ids": [
        "chunk_5e6f",
        "chunk_9g0h"
      ]
    },
    {
      "id": "multi_head_q3",
      "question": "Why does multi-head attention outperform single-head attention?",
      "document_id": 123,
      "ground_truth_chunk_ids": [
        "chunk_7i8j",
        "chunk_2k3l"
      ]
    },
    {
      "id": "positional_encoding_q4",
      "question": "What is the purpose of sinusoidal positional encodings in the Transformer?",
      "document_id": 123,
      "ground_truth_chunk_ids": [
        "chunk_1m4n",
        "chunk_6o5p"
      ]
    },
    {
      "id": "training_details_q5",
      "question": "What was the BLEU score achieved by the Transformer model on WMT 2014 English-to-German translation?",
      "document_id": 123,
      "ground_truth_chunk_ids": [
        "chunk_9q0r"
      ]
    }
  ]